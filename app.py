"""The main module of the app.

Contains only the corpus of the app.
All the functionalities are imported.
"""

import streamlit as st
import os

# Import modules
from viz import create_vae_diagram
from train import train_model

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
from typing import Tuple, Dict, List
import streamlit as st
import time

import streamlit as st
import matplotlib.pyplot as plt
import torch
import os
import torch.nn as nn

# Import des modules personnalis√©s
from vae import VAE
from utils import get_dataset, generate_model_id
from viz import plot_loss_curves, visualize_reconstructions, generate_samples


# Configuration de la page
st.set_page_config(
    page_title="Variational Autoencoder Explorer", page_icon="üñº", layout="wide"
)

# Assurez-vous que le dossier 'models' existe
if not os.path.exists("models"):
    os.makedirs("models")

# Structure principale de l'application avec deux colonnes
col1, col2 = st.columns([3, 1])


# Fonction pour cr√©er et entrainer le mod√®le ou charger un mod√®le existant
def modelisation(
    dataset, latent_dim, hidden_layers, reconstruction_error, batch_size, epochs
):
    # Cr√©ation du dataloader
    train_loader, test_loader, dim = get_dataset(dataset, batch_size)

    # Param√®tres du mod√®le pour identifier les mod√®les d√©j√† entra√Æn√©s
    model_params = {
        "dataset": dataset,
        "latent_dim": latent_dim,
        "hidden_layers": hidden_layers,
        "reconstruction_error": reconstruction_error,
        "epochs": epochs,
    }

    model_id = generate_model_id(model_params)
    model_path = f"models/vae_{model_id}.pth"
    losses_path = f"models/vae_{model_id}_losses.pth"

    # Cr√©ation du mod√®le VAE
    c, w, h = dim
    model = VAE(
        c,
        w,
        h,
        latent_dim=latent_dim,
        hidden_dims=hidden_layers,
        recon_loss_type=reconstruction_error,
    )

    # Container pour les visualisations apr√®s l'entra√Ænement
    results_container = st.container()

    # V√©rifier si le mod√®le existe d√©j√†
    if os.path.exists(model_path) and os.path.exists(losses_path):
        st.info(f"Chargement d'un mod√®le existant avec les m√™mes param√®tres...")

        # Charger le mod√®le et les historiques de perte
        model.load_state_dict(torch.load(model_path, map_location=torch.device("cpu")))
        losses = torch.load(losses_path, map_location=torch.device("cpu"))

        # Afficher le r√©capitulatif des pertes
        with results_container:
            st.subheader("Historique d'entra√Ænement")
            fig_loss, ax_loss = plt.subplots(figsize=(10, 4))
            # Ajouter les points pour chaque epoch
            ax_loss.plot(losses["train_losses"], "b-", label="Train Loss")
            ax_loss.scatter(
                range(len(losses["train_losses"])), losses["train_losses"], color="blue"
            )
            ax_loss.plot(losses["test_losses"], "r-", label="Test Loss")
            ax_loss.scatter(
                range(len(losses["test_losses"])), losses["test_losses"], color="red"
            )
            ax_loss.set_xlabel("Epoch")
            ax_loss.set_ylabel("Loss")
            ax_loss.legend()
            ax_loss.grid(True)
            st.pyplot(fig_loss)
            plt.close(fig_loss)

            # Afficher message de confirmation
            st.success(f"Mod√®le charg√© avec succ√®s! (Nombre d'epochs: {epochs})")
    else:
        # Affichage barre de progression
        progress_container = st.container()
        with progress_container:
            progress_bar = st.progress(0)
            status_text = st.empty()

        # Container pour les visualisations pendant l'entra√Ænement
        training_vis_container = st.container()
        with training_vis_container:
            # Placeholders pour les graphiques mis √† jour
            loss_placeholder = st.empty()
            images_placeholder = st.empty()

        # Entrainement du mod√®le
        trained_model, losses = train_model(
            model=model,
            train_loader=train_loader,
            test_loader=test_loader,
            num_epochs=epochs,
            progress_bar=progress_bar,
            status_text=status_text,
            loss_placeholder=loss_placeholder,
            images_placeholder=images_placeholder,
        )

        # Sauvegarder le mod√®le entra√Æn√© et les historiques de pertes
        torch.save(model.state_dict(), model_path)
        torch.save(losses, losses_path)

        # Afficher un message de fin d'entrainement
        st.success(
            f"Entrainement termin√© apr√®s {epochs} epochs! Mod√®le sauvegard√© dans {model_path}"
        )

    # Afficher des visualisations dans le conteneur de r√©sultats
    with results_container:
        # Obtenir un batch de test diff√©rent pour la reconstruction
        test_batch_for_recon = next(iter(test_loader))[0]

        # Afficher les images g√©n√©r√©es
        st.subheader("Images g√©n√©r√©es")
        fig_gen = plt.figure(figsize=(10, 10))
        generate_samples(model, num_samples=16, fig=fig_gen)
        st.pyplot(fig_gen)
        plt.close(fig_gen)

        # Afficher des reconstructions
        st.subheader("Reconstructions (sur un autre batch)")
        fig_recon = plt.figure(figsize=(10, 5))
        visualize_reconstructions(
            model, test_batch_for_recon, num_images=4, fig=fig_recon
        )
        st.pyplot(fig_recon)
        plt.close(fig_recon)

    return model


# Sidebar pour les hyperparam√®tres
st.sidebar.title("Param√®tres du VAE")

# S√©lection du dataset
dataset = st.sidebar.selectbox("Dataset", ["MNIST", "CIFAR10"])

# Dimension de l'espace latent
latent_dim = st.sidebar.slider(
    "Dimension de l'espace latent", min_value=2, max_value=200, value=40, step=1
)

# Couches convolutionnelles cach√©es
hidden_layers = st.sidebar.multiselect(
    "Couches convolutionnelles cach√©es",
    options=[32, 64, 128, 256, 512],
    default=[32, 64],
)

# Type d'erreur de reconstruction
reconstruction_error = st.sidebar.selectbox(
    "Erreur de reconstruction", ["gaussian", "laplace"]
)

# Taille du batch
batch_size = st.sidebar.selectbox("Taille du batch", options=[64, 128, 256], index=0)

# Nombre d'epochs
epochs = st.sidebar.slider(
    "Nombre d'epochs", min_value=1, max_value=50, value=5, step=1
)

if dataset.lower() == "mnist":
    st.sidebar.info(
        """
    **MNIST** : 60 000 images d'entra√Ænement et 10 000 images de test
    de chiffres manuscrits (0-9) en noir et blanc de dimensions 1 canaux et 28x28 pixels.
    """
    )
if dataset.lower() == "cifar10":
    st.sidebar.info(
        """
    **Cifar10** : 50 000 images d'entra√Ænement et 10 000 images de test
    reparti en 10 classes (avion, automobile, oiseau, chat, cerf, chien, grenouille, bateau, camion) en couleur
    de dimensions 3 canaux et 32x32 pixels.
    """
    )

# Colonne 2: Sch√©ma du VAE (toujours visible)
with col2:
    st.markdown(4 * "<br>", unsafe_allow_html=True)
    st.subheader("Architecture")

    # D√©terminer le nombre de canaux bas√© sur le dataset s√©lectionn√©
    dim = (1, 28, 28) if dataset.lower() == "mnist" else (3, 32, 32)

    # Cr√©er et afficher le sch√©ma du VAE qui est interactif avec les param√®tres
    vae_diagram = create_vae_diagram(
        input_dim=dim, latent_dim=latent_dim, hidden_dims=hidden_layers
    )
    st.graphviz_chart(vae_diagram)

# Colonne 1: Contenu principal
with col1:
    st.title("Explorateur de Variational Autoencoder (VAE)")
    st.write(
        """
    Cette application permet d'explorer les Variational Autoencoders (VAEs), un type de mod√®le g√©n√©ratif
    qui apprend √† reconstruire et g√©n√©rer des images √† partir d'un espace latent continu.
    Ajustez les param√®tres dans la barre lat√©rale et lancez l'entrainement pour observer les r√©sultats.
    """
    )

    # Section "En savoir plus sur les VAE"
    with st.expander("En savoir plus sur les VAE"):
        st.write(
            """
        ### Qu'est-ce qu'un Variational Autoencoder (VAE)?
        
        Un Variational Autoencoder est un type de r√©seau de neurones g√©n√©ratif qui apprend √† repr√©senter des donn√©es
        dans un espace latent continu. Contrairement aux autoencoders classiques, les VAEs imposent une structure sur
        l'espace latent en utilisant une approche probabiliste.
        
        ### Architecture
        
        Un VAE se compose de deux parties principales:
        
        1. **Encodeur**: Transforme les donn√©es d'entr√©e en distributions dans l'espace latent (moyenne Œº et variance œÉ¬≤)
        2. **D√©codeur**: Reconstruit les donn√©es √† partir d'√©chantillons de l'espace latent
        
        ### Fonction de perte
        
        La fonction de perte d'un VAE comprend deux termes:
        
        - **Erreur de reconstruction**: Mesure la diff√©rence entre les donn√©es d'entr√©e et leur reconstruction
        - **Divergence KL**: Force la distribution latente √† se rapprocher d'une distribution normale standard
        
        La fonction de perte totale est: L = Reconstruction_Loss + Œ≤ * KL_Divergence
        
        ### G√©n√©ration de nouvelles donn√©es
        
        Apr√®s l'entrainement, on peut g√©n√©rer de nouvelles donn√©es en:
        1. √âchantillonnant des points de l'espace latent suivant N(0, I)
        2. D√©codant ces points en utilisant le d√©codeur
        
        ### Applications
        
        Les VAEs sont utilis√©s pour:
        - La g√©n√©ration d'images
        - La compression de donn√©es
        - L'apprentissage de repr√©sentations
        - L'interpolation entre diff√©rentes donn√©es
        """
        )

    # Bouton pour lancer l'entrainement
    if st.button("Entrainer le mod√®le"):
        with st.spinner("Entrainement/chargement du mod√®le en cours..."):
            trained_model = modelisation(
                dataset=dataset,
                latent_dim=latent_dim,
                hidden_layers=hidden_layers,
                reconstruction_error=reconstruction_error,
                batch_size=batch_size,
                epochs=epochs,
            )

    # Section R√©f√©rences
    st.markdown("---")
    st.subheader("R√©f√©rences")
    st.markdown(
        """
    - [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114) - Diederik P. Kingma, Max Welling (2013)
    - [Tutorial on Variational Autoencoders](https://arxiv.org/abs/1606.05908) - Carl Doersch (2016)
    """
    )

    # Footer
    st.markdown("---")
    st.markdown(
        "D√©velopp√©e par [Lylian Challier](https://github.com/LylianChallier?tab=repositories) et Mohammed-Amine Grini avec Streamlit et Pytorch."
    )
